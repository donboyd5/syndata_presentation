---
title: "Synthetic PUF presentation"
author: Don Boyd
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook: 
    df_print: paged
    toc: yes
    toc_depth: 5
    number_sections: true
editor_options: 
  chunk_output_type: console
---

<!--
  Enclose comments for RMD files in these kinds of opening and closing brackets
-->

**CAUTION: Remember to read tax data files with default column type of double to avoid loss of precision.**


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


```{r clear_warnings, eval=FALSE}
# run this line to clear any old warning messages
assign("last.warning", NULL, envir = baseenv())

```


```{r system_specific_info, include=TRUE}
# change information here as needed

slgfdir <- "C:/Users/donbo/Dropbox/SLGF/taxdata_synpuf/" # enhanced files are stored here
syndrop <- "C:/Users/donbo/Dropbox/synpuf/"

pufpath <- "C:/Users/donbo/Dropbox/OSPC - Shared/IRS_pubuse_2011/puf2011.csv"

# original files prepped for tax calculator
osr_path <- paste0(slgfdir, "ostack_rotten.csv")
osr_samp_path <- paste0(slgfdir, "ostack_rotten.csv")

# synthetic files
syndir <- "C:/Users/donbo/Google Drive/synpuf/syntheses/"

synfn <-  "synpuf20_no_disclosures_weighted.csv"
syndanfn <- "synpuf20calc.csv"
synfn_td <-  "synpuf20_no_disclosures_weighted_for_taxdata.csv"

# synpath <- paste0(syndrop, synfn)
# danpath <- paste0(syndrop, syndanfn)
syntdpath <- paste0(syndrop, synfn_td)


# TaxData enhanced data ----
epuf_fn <- "puf_rottenpuf.csv"
esyn_fn <- "puf_synpuf.csv"

epuf_path <- paste0(slgfdir, epuf_fn)
esyn_path <- paste0(slgfdir, esyn_fn)

taxout <- "D:/tax_data/" # Tax-Calculator results go here

```


```{r includes, include=FALSE}
source(here::here("r/", "libraries.r"))
source(here::here("r/", "functions.r"))

```


# Compare unweighted files
```{r define_comparison_files, include=FALSE}
puf <- read_csv(pufpath)
glimpse(puf)

syn <- read_csv(syntdpath)
glimpse(syn)

puf_vnames <- get_puf_vnames() %>% mutate(vname=str_to_upper(vname)) %>% filter(vnum!=9030) # remove a 2nd XTOT

common_names <- intersect(names(puf), names(syn))

stack <- puf %>%
  select(common_names) %>%
  filter(RECID < 999996) %>%
  mutate(ftype="puf") %>%
  bind_rows(syn %>% select(common_names) %>% mutate(ftype="syn"))

epvars <- names(stack)[(names(stack) %>% str_sub(., 1, 1) %in% c("E", "P"))] 
epvars

pufsums <- puf %>%
  select(RECID, S006, epvars) %>%
  pivot_longer(cols=-c(RECID, S006)) %>%
  group_by(vname=name) %>%
  summarise(wtdsumb=sum(S006 / 100 * value) / 1e9) %>%
  left_join(puf_vnames %>% select(vname, vdesc, category)) %>%
  mutate(category=ifelse(category=="agi", "income", category)) %>%
  arrange(desc(wtdsumb))

largeinc <- c("E00100", "E00200", "E01700", "E02000")
largeded <- c("E19200", "E18400", "E19800", "E17500")

# xcheck <- puf %>% select(RECID, MARS, starts_with("X")) %>% mutate(ftype="puf") %>%
#   bind_rows(syn %>% select(RECID, MARS, starts_with("X")) %>% mutate(ftype="syn"))
# 
# xtab <- xcheck %>%
#   filter(ftype=="puf", RECID < 999996) %>%
#   gather(xvar, value, starts_with("X")) %>%
#   group_by(MARS, xvar, value) %>%
#   summarise(n=n()) %>%
#   mutate(value=paste0("numexempt_", value)) %>%
#   spread(value, n) %>%
#   mutate_at(vars(starts_with("num")), list(~naz(.)))
# xtab %>% write_csv(here::here("data", "xtab.csv"))
# xtab %>%
#   kable(digits=0, format.args = list(big.mark=",")) %>%
#   kable_styling()

```


```{r wtd_sums, eval=FALSE}
vnames <- c("E00100", "E00200")
wsum <- function(vnames, scale=1e6) {
  stack %>%
    mutate(nret=1) %>%
    select(ftype, S006, vnames) %>%
    mutate(wt=S006 / 100) %>%
    pivot_longer(cols=-c(ftype, wt, S006), names_to="vname", values_to = "value") %>%
    group_by(ftype, vname) %>%
    summarise(wtdsum=sum(value * wt) / scale) %>%
    pivot_wider(names_from=ftype, values_from=wtdsum) %>%
    mutate(diff=syn - puf,
           pdiff=diff / puf * 100)
}
wvals <- wsum(c("nret", "E00100", "E00200", "E00300"))
wvals %>% kable(digits=c(0, 0, 0, 0, 1),
          format.args=list(big.mark = ',')) %>%
  kable_styling()
```


## Univariate stats

```{r univariate_stats}

a <- proc.time()
long <- stack %>%
  select(ftype, starts_with("E"), starts_with("P"), -EIC) %>%
  pivot_longer(cols=-ftype, names_to="vname", values_to="value") # 8.5 secs
  # gather(vname, value, -ftype) # 4.3 secs
b <- proc.time()
b - a
glimpse(long)

a <- proc.time()
stats <- long %>%
  group_by(ftype, vname) %>%
  summarise(n=n(),
            n.NA=sum(is.na(value)),
            nnz=sum(value != 0),
            nnz_pct=nnz / n * 100,
            mean=mean(value, na.rm=TRUE),
            median=median(value, na.rm=TRUE), 
            sd=sd(value, na.rm=TRUE),
            kurtosis=e1071::kurtosis(value, type=2, na.rm=TRUE),
            skewness=e1071::skewness(value, type=2, na.rm=TRUE))
b <- proc.time() # 40 secs
b - a
stats %>% ht

stats_lab <- stats %>%
  left_join(puf_vnames %>% select(vname, vdesc)) %>%
  left_join(pufsums %>% select(vname, wtdsumb))

a <- proc.time()
nzstats <- long %>%
  filter(value!=0) %>%
  group_by(ftype, vname) %>%
  summarise(n=n(),
            n.NA=sum(is.na(value)),
            nneg=sum(value < 0),
            neg_pct=nneg / n * 100,
            mean=mean(value, na.rm=TRUE),
            median=median(value, na.rm=TRUE), 
            sd=sd(value, na.rm=TRUE),
            kurtosis=e1071::kurtosis(value, type=2, na.rm=TRUE),
            skewness=e1071::skewness(value, type=2, na.rm=TRUE))
b <- proc.time() # 40 secs
b - a
nzstats_lab <- nzstats %>%
  left_join(puf_vnames %>% select(vname, vdesc)) %>%
  left_join(pufsums %>% select(vname, wtdsumb))
nzstats_lab %>% ht

# make a wide file of statistics
nzwstats <- nzstats_lab %>%
  select(-n, -n.NA, -nneg) %>%
  gather(stat, value, -ftype, -vname, -vdesc, -wtdsumb) %>%
  spread(ftype, value) %>%
  mutate(stat=factor(stat, levels=c("neg_pct", "mean", "median", "sd", "kurtosis", "skewness"))) %>%
  arrange(vname, stat) %>%
  mutate(diff=syn - puf,
         pdiff=diff / puf * 100)

fstat <- function(vname.in){
  tab <- nzwstats %>%
    filter(vname==vname.in) %>%
    select(vname, stat, puf, syn, diff, pdiff, vdesc) %>%
    kable(digits=c(0, 0, 0, 0, 0, 1, 0, 0),
          format.args=list(big.mark = ','),
          format="rst")
  # %>% kable_styling()
  # print(tab)
  return(tab)
}

# fstat("E00100")
# fstat("E00200")
# fstat("E00300")
# fstat("E01500")
fstat("E02000")
fstat(largeded[4])
largeinc
# 
# wstats %>%
#   mutate(diff=syn - puf,
#          pdiff=diff / puf * 100) %>%
#   kable(digits=c(0, 0, 1, 1, 1, 1),
#         format.args=list(big.mark = ','))
# ht(wstats)
  # tab <- nzwstats %>%
  #   filter(vname==vname.in) %>%
  #   select(vname, stat, puf, syn, diff, pdiff, vdesc) %>%
vname.in <- largeinc[2]
ft <- nzwstats %>%
  filter(vname==vname.in) %>%
  select(stat, puf, syn, diff, pdiff) %>%
  flextable() %>% 
  set_header_labels(stat="Statistic",
                    puf = "PUF",
                    syn = "Synthetic\nfile",
                    diff="Difference",
                    pdiff="% difference") %>%
  colformat_num(col_keys = c("puf", "syn", "diff"),
                big.mark=",", digits = 0, na_str = "N/A") %>%
  colformat_num(col_keys = c("pdiff"),
                big.mark=",", digits = 1, na_str = "N/A") %>%
  add_header_lines(values = vname.in) %>%
  theme_box() %>%
  autofit()#  %>% # dim width(j=~variable1 + variable2, width=3)
ft
dim(ft)
# save_as_image(ft, path = here::here("results", "corr_worst_table.png"))


# stats scatter ----
vstat <- "median"
vstat <- "sd"
vstat <- "kurtosis"
vstat <- "skewness"
vstat <- "mean"
nzwstats %>%
  filter(stat==vstat) %>%
  mutate(vlab=ifelse(abs(pdiff>20), vname, NA)) %>%
  ggplot(aes(puf, syn)) +
  geom_point() +
  geom_text(aes(label=vlab)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_abline(slope=1, intercept=0) +
  ggtitle(vstat) +
  coord_equal()



```


### Plot of nonzero percent 
```{r}
nzdat <- stats_lab %>%
  mutate(z_pct=100 - nnz_pct) %>%
  select(ftype, vname, vdesc, wtdsumb, z_pct) %>%
  pivot_wider(names_from = ftype, values_from=z_pct) %>%
  mutate(diff=syn - puf)


f <- function(df) {
  df <- df %>%
    filter(vname %in% vars) %>%
    mutate(vname=factor(vname, levels=vars, labels=vlabs))
  return(df)
}

vars <- c("E00100", "E00200", "E00300", "E00600", "E01400")
vlabs <- c("AGI\n(E00100)", "Wages\n(E00200)", "Interest\nreceived\n(E00300)", "Dividends\n(E00600)",
           "Taxable IRA\ndistribution\n(E01400)")

p <- stats_lab %>%
  mutate(z_pct=100 - nnz_pct) %>%
  select(ftype, vname, vdesc, z_pct) %>%
  pivot_wider(names_from = ftype, values_from=z_pct) %>%
  mutate(labval=factor(vname, levels=vars, labels=vlabs),
         labeled=ifelse(vname %in% vars, 1, 0) %>% as.factor) %>%
  ggplot(aes(x=puf, y=syn, label=labval)) +
  geom_point(aes(colour=labeled, size=labeled)) +
  scale_colour_manual(values=c("black", "darkblue")) +
  scale_size_manual(values=c(0.2, 0.8)) +
  geom_abline(slope=1, intercept=0) +
  geom_text_repel(size=2, fontface = "bold", colour="darkblue", point.padding = 0.2, nudge_x=3) + # point.padding = 0.005, 
  # geom_text(size=3, fontface = "bold", colour="darkblue", hjust=1, nudge_x=-3, nudge_y=0) +
  coord_equal() +
  ggtitle("Percentage of observations that are zero",
          subtitle="Selected important variables labeled") +
  theme_bw() +
  scale_x_continuous(name="% of PUF observations", limits=c(0, 100), breaks=seq(0, 100, 10)) +
  scale_y_continuous(name="% of synthetic observations", limits=c(0, 100), breaks=seq(0, 100, 10))  +
  theme(legend.position = "none")
p
ggsave(here::here("results", "scat_zpct.png"), p, width=9, height=5)

```




### Density plots
```{r}
var <- "E00100"
p <- stack %>%
  dplyr::select(ftype, value=var) %>%
  filter(value > 1000) %>%
  mutate(grp=cut(value, breaks=c(-Inf, 0, 10e3, 25e3, 50e3, 75e3, 100e3, 250e3, 1e6, 10e6, Inf)),
         ftype=factor(ftype, levels=c("puf", "syn"))) %>%
  ggplot(aes(value, colour=ftype)) +
  geom_density(size=1.5) +
  scale_colour_manual(values=c("blue", "red")) +
  # scale_x_continuous(name="value of variable") +
  # xscale.l10 +
  theme_bw() +
  ggtitle(var, subtitle="Only includes values above $1,000") +
  theme(plot.title=element_text(size=12)) +
  theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
  facet_wrap(~grp, ncol=3, scales="free")
p

qqnorm(stack$E00100[stack$ftype=="puf"])

```


## Kernel density plots, selected variables
```{r uvplots_kernel, include=FALSE}
var <- "E00100"
kdplot <- function(var, vnames=puf_vnames){
  sq10 <- c(0, 1e3, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6,
            1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
  xlabs <- scales::comma(sq10 / 1e3)
  
  xscale.l10 <- scale_x_log10(name=paste0(var, " in $ thousands, log 10 scale"), breaks=sq10, labels=xlabs)
  
  vdesc <- vnames$vdesc[vnames$vname==var]
  
  gtitle <- paste0("Kernel density plot for: ", vdesc, " (", var, ")")
  
  p <- stack %>%
    dplyr::select(ftype, value=var) %>%
    filter(value > 1000) %>%
    mutate(ftype=factor(ftype, levels=c("puf", "syn"))) %>%
    ggplot(aes(value, colour=ftype)) +
    geom_density(size=1.5) +
    scale_colour_manual(values=c("blue", "red")) +
    # scale_x_continuous(name="value of variable") +
    xscale.l10 +
    theme_bw() +
    ggtitle(gtitle, subtitle="Only includes values above $1,000") +
    theme(plot.title=element_text(size=12)) +
    theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black"))
  print(p)
  return(p)
}


kdplot_fill <- function(var, vnames=puf_vnames){
  sq10 <- c(0, 1e3, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6,
            1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
  xlabs <- scales::comma(sq10 / 1e3)
  
  # xscale.l10 <- scale_x_log10(name=paste0(var, " in $ thousands, log 10 scale"), breaks=sq10, labels=xlabs)
  xscale.l10 <- scale_x_log10(name=paste0(var, " in $ thousands, log 10 scale"), breaks=sq10, labels=xlabs, limits=c(1e3, NA))
  
  vdesc <- vnames$vdesc[vnames$vname==var]
  
  gtitle <- paste0("Kernel density plot for positive values of: ", vdesc, " (", var, ")")
  
  p <- stack %>%
    dplyr::select(ftype, value=var) %>%
    filter(value > 0) %>%
    mutate(ftype=factor(ftype, levels=c("puf", "syn"))) %>%
    group_by(ftype) %>%
    # sample_n(1000) %>%
    ggplot(aes(value, fill=ftype)) +
    geom_density(size=0.0, alpha=0.2) +
    scale_fill_manual(values=c("blue", "red")) +
    # scale_fill_manual(values=c("blue", "#de2d26")) +
    # scale_x_continuous(name="value of variable") +
    xscale.l10 +
    theme_bw() +
    ggtitle(gtitle, subtitle="Log scale; values below $1,000 not shown") +
    theme(plot.title=element_text(size=12)) +
    theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
    guides(fill = guide_legend(title=NULL))
  print(p)
  return(p)
}

# geom_density defaults
# bw = "nrd0", adjust = 1, kernel = "gaussian", n = 512, trim = FALSE
# bw The smoothing bandwidth to be used. If numeric, the standard deviation of the smoothing kernel.
# If character, a rule to choose the bandwidth, as listed in stats::bw.nrd().
# bw.nrd0 implements a rule-of-thumb for choosing the bandwidth of a Gaussian kernel density estimator. It defaults
# to 0.9 times the minimum of the standard deviation and the interquartile range divided by 1.34 times the sample size
# to the negative one-fifth power (= Silverman's ‘rule of thumb’, Silverman (1986, page 48, eqn (3.31))) 
# unless the quartiles coincide when a positive result will be guaranteed. 

# n number of equally spaced points at which the density is to be estimated, should be a power of two
# When n > 512, it is rounded up to a power of 2 during the calculations (as fft is used) and the final result is 
# interpolated by approx. So it almost always makes sense to specify n as a power of two.

```


### Income variables, kernel density plots
```{r uvplots_kd_income}
p1 <- kdplot_fill(largeinc[1])
(p1a <- p1 + annotate("text", x=5e6, y=.45, label="AGI", size=8))

p2 <- kdplot_fill(largeinc[2])
(p2a <- p2 + annotate("text", x=2e6, y=.5, label="Wages", size=8))

p3 <- kdplot_fill(largeinc[3])
(p3a <- p3 + annotate("text", x=500e3, y=.6, label="Pensions", size=8))

p4 <- kdplot_fill(largeinc[4])
(p4a <- p4 + annotate("text", x=7.5e3, y=.35, label="Schedule E\nNet income", size=8))

g <- arrangeGrob(p1a, p2a, p3a, p4a, ncol=2)
ggsave(here::here("results", "kdf_income.png"), g, width=9, height=5, scale=1.75)

# kdplot(var="E00200") #, puf_vnames)
# l_ply(incvars, kdplot)
```


### Deduction variables, kernel density plots
```{r uvplots_kd_deduct}
q1 <- kdplot_fill(largeded[1])
q1a <- q1 + annotate("text", x=250e3, y=.75, label="Interest paid", size=8)

q2 <- kdplot_fill(largeded[2])
q2a <- q2 + annotate("text", x=1e6, y=.4, label="SALT", size=8)

q3 <- kdplot_fill(largeded[3])
q3a <- q3 + annotate("text", x=500e3, y=.5, label="Cash\ncontributions", size=8)

q4 <- kdplot_fill(largeded[4])
q4a <- q4 + annotate("text", x=100e3, y=1, label="Medical &\ndental", size=8)

gq <- arrangeGrob(q1a, q2a, q3a, q4a, ncol=2)
ggsave(here::here("results", "kdf_deductions.png"), gq, width=9, height=5, scale=1.75)

# kdplot(var="E00200") #, puf_vnames)
# l_ply(incvars, kdplot)
```


### Distribution similarity
```{r}
glimpse(stack)
ns(stack)
glimpse(long)
count(long, vname)

# get the puf relative frequencies
ngroups <- 100
rfreq_puf <- long %>%
  filter(ftype=="puf") %>%
  # filter(vname %in% c("E00100", "E00200")) %>%
  group_by(vname) %>%
  mutate(grp=ntile(value, ngroups)) %>%
  group_by(vname, grp) %>%
  summarise(n=n(), minval=min(value), maxval=max(value))

rfreq_puf %>% filter(vname=="E00100") %>% ht

# add breakpoints to the file
rfreq_pufbrk <- rfreq_puf %>%
  group_by(vname) %>%
  mutate(lower=ifelse(row_number()==1,
                      -Inf, 
                      (minval + lag(maxval)) /2),
         upper=ifelse(row_number()==n(),
                      Inf, (maxval + lead(minval)) / 2))

rfreq_pufbrk %>% filter(vname=="E00200") %>% ht

# make unique groups
pufgroups <- rfreq_pufbrk %>%
  group_by(vname, lower) %>%
  summarise(n=sum(n), upper=max(upper)) %>%
  group_by(vname) %>%
  # arrange(lower) %>%
  mutate(rfreq=n / sum(n)) %>%
  # ungroup %>%
  mutate(groupnum=rank(lower)) %>%
  select(vname, groupnum, lower, upper, n, rfreq)

# mutate(groupnum=group_indices(.dots="lower"))

pufgroups %>%
  group_by(vname) %>%
  filter(rfreq > .015)
# wages has 20% of obs in one group


```

```{r djb}
# now that we have pufgroups we can compute syn relative frequencies for the same groups
# and see how they compare
df <- long %>%
  filter(ftype=="syn") %>%
  filter(vname %in% c("E00100", "E00200")[2])

i<-20; df$value[i]; df$valgroup[i] %>% as.character(); df$binnum[i]
tmp <- pufgroups %>% filter(vname=="E00200")

ht(brks)
cut(c(-100, 0, 1, 1000, 3.114e6, 4e6, Inf), brks, include.lowest = TRUE, right=FALSE)

getbin <- function(df){
  brks <- c(-Inf, pufgroups$upper[pufgroups$vname==df$vname[1]])
  df$valgroup <- cut(df$value, brks, include.lowest = TRUE)
  df$binnum <- as.integer(df$valgroup)
  df$valgroup <- as.character(df$valgroup) # factors don't work once we combine with other variables with different levels
  return(df)
}

rfreq_syn <- long %>%
  filter(ftype=="syn") %>%
  filter(vname %in% c("E00100", "E00200")[2]) %>%
  group_by(vname) %>%
  do(getbin(.))



# determine the bin for each observation
a <- proc.time()
bindat <- stack %>%
  # sample_n(100) %>%
  dplyr::select(ftype, binvars) %>%
  gather(vname, value, -ftype) %>%
  group_by(ftype, vname) %>%
  do(getbin(.)) %>%
  ungroup %>%
  arrange(ftype, vname, binnum)
b <- proc.time()
b - a # 2-3 minutes

# collapse by ftype, vname, and bin
binned <- bindat %>%
  group_by(ftype, vname) %>%
  mutate(ntot=n()) %>%
  group_by(ftype, vname, binnum, valgroup) %>%
  summarise(n=n(), ntot=first(ntot)) %>%
  mutate(pct=n / ntot * 100)

# get ssd by ftype, vname
a <- proc.time()
bincomp <- binned %>%
  group_by(vname, binnum, valgroup) %>%
  mutate(diff=pct - pct[ftype=="puf"],
         diffsq=diff^2) %>%
  group_by(ftype, vname) %>%
  summarise(ssd=sum(diffsq)) %>%
  filter(ftype!="puf")
b <- proc.time()
b - a






# play below here ----

tmp <- density(stack %>% mutate(value=E00650) %>% filter(ftype=="puf", value>0) %>% mutate(value=log(value)) %>% .[["value"]])
# str(tmp)
tibble(x=tmp$x, y=tmp$y) %>%
  ggplot(aes(x, y)) +
  geom_line()

df <- stack
vname <- "E00200"

den <- function(df, vname.in){
  # df is a long data file
  npoints <- 512
  
  pufvals <- df %>% filter(ftype=="puf", vname==vname.in) %>% .[["value"]]
  pufmin <- min(pufvals)
  pufmax <- max(pufvals)
  synvals <- df %>% filter(ftype=="syn", vname==vname.in) %>% .[["value"]]
  
  if(length(pufvals) < 10 | length(synvals) < 10) {
    return(tibble(i=1:npoints, x=NA, puf=NA, syn=NA))
  }
  pufden <- density(pufvals, from=pufmin, to=pufmax, n=npoints)
  synden <- density(synvals, from=pufmin, to=pufmax, n=npoints)
  
  dendf <- tibble(i=1:npoints, x=pufden$x, puf=pufden$y, syn=synden$y)
  return(dendf)
}
tmp <- den(long, "E00200")

tmp <- long %>%
  # filter(vname=="E00200") %>%
  group_by(vname) %>%
  do(den(., .$vname))

tmp %>%
  select(x, puf, syn) %>%
  mutate(ygrp=cut(x, 8)) %>% 
  gather(ftype, den, puf, syn) %>%
  # filter(pufx < 100e3) %>%
  ggplot(aes(x, den, colour=ftype)) + geom_line() + facet_wrap(~ygrp, ncol=2, scales="free")

tmp %>%
  select(x, puf, syn) %>%
  mutate(ygrp=cut(x, 4)) %>% 
  gather(ftype, den, puf, syn) %>%
  # filter(pufx < 100e3) %>%
  ggplot(aes(x, den, colour=ftype)) + geom_line()

glimpse(tmp)
tmp %>%
  group_by(vname) %>%
  summarise(dist=sum((puf - syn)^2) %>% sqrt) %>%
  arrange(-dist) %>%
  left_join(pufsums %>% select(vname, wtdsumb, vdesc)) %>%
  kable(digits=4)


# library(overlapping)
set.seed(20150605)
# overlap(x, nbins = 1000, plot = FALSE, partial.plot = FALSE)
x <- list(X1=rnorm(100),X2=rt(50,8),X3=rchisq(80,2))
out <- overlap(x, plot=TRUE)



x <- list(X1=rnorm(100),X2=rt(50,8),X3=rchisq(80,2))
var <- "E00100"
x <- list(puf=stack %>% filter(ftype=="puf") %>% .[[var]],
          syn=stack %>% filter(ftype=="syn") %>% .[[var]])

x <- list(puf=stack %>% select(ftype, value=var) %>% 
            filter(ftype=="puf", value>0) %>% mutate(value=log(value)) %>% .[["value"]],
          syn=stack %>% select(ftype, value=var) %>% 
            filter(ftype=="syn", value>0) %>% mutate(value=log(value)) %>%  .[["value"]])

out <- overlap(x, plot=TRUE)
str(out)
out$OV
unname(out$OV)

olap <- function(df, vname.in){
  print(vname.in[1])
  pufvals <- df %>% filter(ftype=="puf", vname==vname.in[1]) %>% .[["value"]]
  synvals <- df %>% filter(ftype=="syn", vname==vname.in[1]) %>% .[["value"]]
  x <- list(puf=pufvals,
            syn=synvals)
  out <- overlap(x)
  # print(str(out))
  return(out$OV %>% unname)
}
glimpse(long)
tmp2 <- long %>%
  # filter(vname=="E00200") %>%
  filter(vname %in% c("E00100", "E00200")) %>%
  group_by(vname) %>%
  summarise(olap=olap(., .$vname))
tmp2

olap <- function(df){
  print(df[1, ])
  vname.in <- df$vname[1]
  print(vname.in)
  pufvals <- df %>% filter(ftype=="puf", vname==vname.in) %>% .[["value"]]
  synvals <- df %>% filter(ftype=="syn", vname==vname.in) %>% .[["value"]]
  x <- list(puf=pufvals,
            syn=synvals)
  out <- overlap(x)
  return(out$OV %>% unname)
}
tmp2 <- long %>%
  # filter(vname=="E00200") %>%
  filter(vname %in% c("E00100", "E00200")) %>%
  group_by(vname) %>%
  summarise(opct=olap(.))
tmp2

olap1 <- function(df){
  vname.in <- df$vname[1]
  
  pufvals <- df %>% filter(ftype=="puf", vname==vname.in) %>% .[["value"]]
  synvals <- df %>% filter(ftype=="syn", vname==vname.in) %>% .[["value"]]
  
  pufvals_nz <- pufvals[pufvals != 0]
  synvals_nz <- synvals[synvals != 0]

  
  x <- list(puf=pufvals,
            syn=synvals)
  olap_all <- overlap(x)
  
  xnz <- list(puf=pufvals_nz,
            syn=synvals_nz)
  olap_nz <- overlap(xnz)
  

  df <- tibble(opct_all=olap_all$OV, 
               opct_nz=olap_nz$OV, 
               puf_nzpct=length(pufvals_nz) / length(pufvals),
               syn_nzpct=length(synvals_nz) / length(synvals))
  return(df)
}

olap2 <- function(df){
  vname.in <- df$vname[1]
  
  pufvals <- df %>% filter(ftype=="puf", vname==vname.in) %>% .[["value"]]
  synvals <- df %>% filter(ftype=="syn", vname==vname.in) %>% .[["value"]]
  
  pufvals_gz <- pufvals[pufvals > 0]
  synvals_gz <- synvals[synvals > 0]

  
  x <- list(puf=pufvals,
            syn=synvals)
  olap_all <- overlap(x)
  
  xgz <- list(puf=pufvals_gz,
            syn=synvals_gz)
  olap_gz <- overlap(xgz)
  
  xlgz <- list(puf=pufvals_gz %>% log(),
            syn=synvals_gz %>% log())
  olap_lgz <- overlap(xlgz)
  

  df <- tibble(opct_all=olap_all$OV, 
               opct_gz=olap_gz$OV,
               opct_lgz=olap_lgz$OV, 
               puf_gpct=length(pufvals_gz) / length(pufvals),
               syn_gzpct=length(synvals_gz) / length(synvals))
  return(df)
}

tmp2 <- long %>%
  # filter(vname=="E00200") %>%
  # filter(vname %in% c("E00100", "E00200")) %>%
  group_by(vname) %>%
  do(olap2(.))

tmp3 <- tmp2 %>%
  ungroup %>%
  mutate(ropct_all=rank(opct_all),
         ropct_gz=rank(opct_gz),
         ropct_lgz=rank(opct_lgz)) %>%
  left_join(pufsums %>% select(vname, wtdsumb, vdesc))

tmp2 %>% 
  arrange(-opct_lgz) %>%
  left_join(pufsums %>% select(vname, wtdsumb, vdesc)) %>%
  ht(10) %>%
  kable(digits=3) %>%
  kable_styling()

tmp2 %>% 
  arrange(opct_nz) %>%
  left_join(pufsums %>% select(vname, wtdsumb, vdesc)) %>%
  kable(digits=3) %>%
  kable_styling()

tmp2 %>%
  ggplot(aes(puf_nzpct, syn_nzpct)) + geom_point()

tmp2 %>%
  ggplot(aes(opct_all, opct_nz)) + geom_point()

tmp2 %>%
  mutate(pctdiff=opct_nz - opct_all) %>%
  ggplot(aes(puf_nzpct, pctdiff)) + geom_point()

# why so little overlap with E02000?
# E02000	0.000	0.699	0.344	0.381	480.567	Schedule E net income or loss (+/-)

# d <- long %>%
#   filter(vname=="E02100") %>%
#   group_by(ftype) %>%
#   summarise(nz=sum(value!=0))

# best E02100 Schedule F net profit/loss (+/-)
# worst E02000 Schedule E net income or loss (+/-)



```


## Bivariate analysis
### Correlations
```{r cor_prepdat, include=FALSE}

(corvars <- epvars)

# create a long file with all correlations
a <- proc.time()
cor1 <- stack %>%
  select(ftype, corvars) %>%
  group_by(ftype) %>%
  do(cordf_new(.[, -1])) %>%
  ungroup %>%
  select(ftype, combo, vname1, vname2, value)
b <- proc.time()
b - a # ~ 50 secs

ht(cor1)
count(cor1, ftype)

cor1 %>%
  filter(str_detect(combo, "E00200")) %>%
  arrange(vname2, ftype)

# create a wide file with correlation comparisons 
# put variable names on the file
cor.comp <- cor1 %>%
  mutate(ftype=factor(ftype, levels=c("puf", "syn"))) %>%
  pivot_wider(names_from=ftype, values_from = value) %>%
  mutate(diff=syn - puf) %>%
  left_join(puf_vnames %>% select(vname1=vname, vdesc1=vdesc)) %>%
  left_join(puf_vnames %>% select(vname2=vname, vdesc2=vdesc))

cor.comp %>%
  filter(str_detect(combo, "E00200")) %>%
  select(-vname1, -vname2) %>%
  arrange(-abs(diff)) %>%
  kable(digits=3) %>%
  kable_styling()

f <- function(df) {
  df <- df %>%
    filter(abs(diff) > .15 | (puf <0 & abs(diff) > .1))
  return(df)
}
p <- cor.comp %>%
  mutate(rank=rank(-abs(puf)),
         outlier=abs(diff) > .15 | (puf <0 & abs(diff) > .1)) %>%
  # filter(rank <= 100) %>%
  # filter(abs(puf) > .05 | abs(syn) > .05) %>%
  ggplot(aes(x=puf, y=syn, label=combo)) +
  geom_point(aes(colour=outlier), size=0.4) +
  geom_abline(slope=1, intercept=0) +
  # geom_text(aes(label=combo), size=2, colour="darkblue", data=f) + # nudge_x=0.06, 
  geom_text_repel(size=3, fontface = "bold", point.padding = 0.005, colour="darkblue", data=f) +
  theme_bw() +
  scale_colour_manual(values=c("black", "red")) +
  scale_x_continuous(name="Correlations within PUF", limits=c(-0.2, 1), breaks=seq(-.2, 1, .2)) +
  scale_y_continuous(name="Correlations within synthetic file", limits=c(-0.2, 1), breaks=seq(-.2, 1, .2)) +
  geom_hline(yintercept = 0, linetype="dotted", size=0.5) +
  geom_vline(xintercept = 0, linetype="dotted", size=0.5) +
  coord_equal() +
  ggtitle("Correlations of variable pairs within each file, compared across files",
          subtitle="Selected outlier correlation pairs labeled") +
  theme(plot.title = element_text(size = 14),
        plot.subtitle = element_text(size = 12)) +
  theme(legend.position = "none")
# p
# ggsave(here::here("results", "scatter.png"), p, width=10, height=6)


# nov 8 ----
# https://github.com/cran/gridExtra/blob/master/vignettes/gtable.Rmd
# vars <- c("E00100", "E00200", "E02000", "E02100", "E07400", "E19800", "E26270", "E58990", "P23250")
outliers <- f(cor.comp)
(vars <- unique(c(outliers$vname1, outliers$vname2)))
vtab <- puf_vnames %>%
  filter(vname %in% vars) %>%
  select(vname, vdesc) %>%
  arrange(vname)
vtab

vtext <- paste0(vtab$vname, ": ", vtab$vdesc) %>% paste(., collapse="\n")
vtext <- paste0("Variables included in labeled correlation pairs:\n\n", vtext)
vtext

g <- arrangeGrob(p, textGrob(vtext, just="left", x=-0.1, y=.5, gp=gpar(fontsize=8)), ncol = 2, widths = c(1.9, 1.1))
grid.newpage(); grid.draw(g)
ggsave(here::here("results", "scat_text.png"), g, width=9, height=5)

# d <- head(iris[,1:3])
# g <- tableGrob(d)
# p <- qplot(1,1)
# p2 <- qplot(3, 4)
# 
# grid.arrange(
#   p,
#   tableGrob(mtcars[1:4, 1:4]),
#   ncol = 2,
#   widths = c(2, 1),
#   clip = FALSE
# )
# 
# # let's make it 5 rows with the plot taking all 3, and 3 columns with the plot taking 2 of the 3
# vtab2 <- vtab %>% mutate(cell=paste0(vname, ": ", vdesc)) %>% select(cell)
# size <- 0.75
# ttheme_tab <- ttheme_minimal(
#     core = list(fg_params=list(cex = size, hjust=0)), # cell size g_params=list(hjust=1, x=0.9)
#     colhead = list(fg_params=list(cex = size)), # colhead text size
#     rowhead = list(fg_params=list(cex = size)))
# layout <- rbind(c(1, 1, NA, NA),
#                 c(1, 1, NA, NA),
#                 c(1, 1, NA, 2),
#                 c(1, 1, NA, NA),
#                 c(1, 1, NA, NA))
# g <- arrangeGrob(p, tableGrob(vtab2, rows=NULL, theme=ttheme_tab),
#             ncol = 2,
#             clip = FALSE,
#             layout_matrix=layout)
# g
# grid.newpage(); grid.draw(g)
# ggsave(here::here("results", "scat_tab.png"), g, width=10, height=6)
# # clip "on", "off" or "inherit"
# 
# grid.arrange(p, textGrob(vtext, just="left", x=0.01, y=.5, gp=gpar(fontsize=8)), ncol = 2, widths = c(2, 1.5))
# grid.arrange(p, textGrob(vtext, just="left", x=0.5, y=.5, gp=gpar(fontsize=8)), ncol = 1, heights = c(2, 1))
# 
# grid.arrange(p, textGrob(vtext, just=c(0, 0), x=0, y=0, gp=gpar(fontsize=12)), ncol = 2, layout_matrix=layout, clip = "off")
# 
# vt2 <- vtab %>% mutate(cell=paste0(vname, ": ", vdesc))
# # https://cran.r-project.org/web/packages/gridExtra/vignettes/tableGrob.html
# t <- textGrob("E00100\nE00200")
# grid.arrange(p, textGrob(vtext, just=c(0, 0)), ncol = 2, widths = c(2, 1), clip="inherit")
# 
# size <- .5
# vt2 <- vtab %>% mutate(cell=paste0(vname, ": ", vdesc))
# ttheme_tab <- ttheme_minimal(
#     core = list(fg_params=list(cex = size, hjust=0)), # cell size g_params=list(hjust=1, x=0.9)
#     colhead = list(fg_params=list(cex = size)), # colhead text size
#     rowhead = list(fg_params=list(cex = size)))
# grid.arrange(p, tableGrob(vt2 %>% select(cell), rows=NULL, theme = ttheme_tab), ncol = 2, widths = c(2, 1), clip = "off")
# 
# 
# size <- .5
# mytheme <- gridExtra::ttheme_default(
#     core = list(fg_params=list(cex = size)), # cell size
#     colhead = list(fg_params=list(cex = size)), # colhead text size
#     rowhead = list(fg_params=list(cex = size)))
# 
# myt <- tableGrob(mtcars[1:5, 1:5], theme = mytheme)
# 
# grid.newpage()
# grid.draw(myt)
# 
# 
# p1 <- ggplot(mtcars, aes(mpg, wt, colour = factor(cyl))) +
#   geom_point()+ theme_article() + theme(legend.position = 'top') 
# p2 <- ggplot(mtcars, aes(mpg, wt, colour = factor(cyl))) +
#   geom_point() + facet_wrap(~ cyl, ncol = 2, scales = "free") +
#   guides(colour = "none") +
#   theme_article()
#   
# ggarrange(p1, p2, widths = c(1.5,2))
# 
# 
# grid.newpage()
# x <- stats::runif(20)
# y <- stats::runif(20)
# rot <- stats::runif(20, 0, 360)
# grid.text("SOMETHING NICE AND BIG", x=x, y=y, rot=rot,
#           gp=gpar(fontsize=20, col="grey"))
# grid.text("SOMETHING NICE AND BIG", x=x, y=y, rot=rot,
#           gp=gpar(fontsize=20), check=TRUE)
# grid.newpage()
# draw.text <- function(just, i, j) {
#   grid.text("ABCD", x=x[j], y=y[i], just=just)
#   grid.text(deparse(substitute(just)), x=x[j], y=y[i] + unit(2, "lines"),
#             gp=gpar(col="grey", fontsize=8))
# }
# x <- unit(1:4/5, "npc")
# y <- unit(1:4/5, "npc")
# grid.grill(h=y, v=x, gp=gpar(col="grey"))
# draw.text(c("bottom"), 1, 1)
# 
# 
# d <- head(iris[,1:3])
# g <- tableGrob(d)
# grid.newpage()
# grid.draw(g)
# 
# q <- grid.arrange(p, g,
#                   nrow=2,
#                   ncol=1,
#                   as.table=TRUE,
#                   heights=c(4, 4),
#                   widths=c(5, 1), respect=FALSE)
# 
# q
# ## non-interactive use, multipage pdf
# ggsave(here::here("results", "scatter3.png"), q)
# 
# a <- gtable(widths=unit(1:3, c("cm")), 
#             heights=unit(5, "cm"))
# 
# a <- gtable(widths=unit(c(4, 1), c("in")), 
#             heights=unit(5, "in"))
# a
# gtable_show_layout(a)
# 
# rect <- rectGrob(gp = gpar(fill = "black"))
# a <- gtable_add_grob(a, p, 1, 1)
# a
# plot(a)
# 
# ggsave(here::here("results", "scatter4.png"), a)
# 
# p <- qplot(1,1)
# p2 <- qplot(3, 4)
# r <- rectGrob(gp=gpar(fill="grey90"))
# t <- textGrob("text")
# grid.arrange(t, p, p2, r, ncol=2)
# 
# 
# d <- head(iris[,1:3])
# g <- tableGrob(d)
# p <- qplot(1,1)
# p2 <- qplot(3, 4)
# r <- rectGrob(gp=gpar(fill="grey90"))
# t <- textGrob("text")
# grid.arrange(g, p, p2, r, ncol=2)
# 
# tmp <- grid.arrange(p, g, ncol=2)
# grid.arrange(p, g, ncol=2, widths=unit(c(7, .5), c("in")), respect=TRUE, clip = "off")
# # plot(tmp)
# a <- gtable(widths=unit(c(4, 1), c("in")), 
#             heights=unit(5, "in"))
# a
# plot(a)
# gtable_show_layout(a)
# a <- gtable_add_grob(a, p, 1, 1)
# a
# plot(a)
# 
# # Transform to a ggplot and print
# library(gridExtra)
# library(ggpubr)
# gt <- arrangeGrob(p, g, ncol = 2, widths=unit(c(7, 1), c("in")))
# q <- as_ggplot(gt)
# q
# ggsave(here::here("results", "scatter5.png"), q)
# 
# 
# 
# # Move to a new page
# d <- head(iris[ , 1:2])
# g <- tableGrob(d, rows=NULL)
# q2 <- as_ggplot(g)
# q2
# 
# grid.newpage()
# # Create layout : nrow = 2, ncol = 2
# pushViewport(viewport(layout = grid.layout(2, 2)))
# # A helper function to define a region on the layout
# # define_region <- function(row, col){
# #   viewport(layout.pos.row = row, layout.pos.col = col, ...)
# # } 
# define_region <- function(row, col, iwidth){
#   viewport(layout.pos.row = row, layout.pos.col = col, width=unit(iwidth, "in"))
# } 
# # Arrange the plots
# print(p, vp=define_region(1:2, 1, iwidth=5))
# print(q2, vp = define_region(1, 2, iwidth=1))
# # print(ydensity, vp = define_region(2, 2))
# 
# 
# print(scatterPlot, vp=define_region(1, 1:2))
# print(xdensity, vp = define_region(2, 1))
# print(ydensity, vp = define_region(2, 2))
# 
# layout(matrix(c(1, 2, 1, 3), ncol=2, byrow=TRUE), widths=c(2, 1))
# ta <- do.call(arrangeGrob, list(p,q2))
# vp <- viewport(height=unit(1, "npc"), width=unit(0.33, "npc"), 
#                just="right", x=1, y=0.5)
# print(ta, vp=vp)
# # https://stackoverflow.com/questions/14358526/controlling-column-widths-for-side-by-side-base-graphic-and-ggplot2-graphic
# 
# 
# layout <- rbind(c(1, 1, 1, 1),
#                 c(2, 2, 3, 3),
#                 c(2, 2, 4, 4))
# grid.arrange(grob1, grob2, grob3, grob4, layout_matrix=layout)
# 
# layout <- rbind(c(1, 2))
# grid.arrange(p, q2, layout_matrix=layout, widths = unit(c(3, 1), "in"))
# 
# grid.arrange(p, q2, layout_matrix=layout, widths = unit(c(0.6, .05), "null"))
# 
# tmp <- arrangeGrob(p, q2, layout_matrix=layout, widths = unit(c(3, 1), "in"))
# grid.show(tmp)
# 
# # All cells are of equal size by default, but users may pass explicity widths and/or heights
# # in any valid grid units, or as relative numbers (interpreted as null),
# # grid.arrange(grobs=gs[1:3], ncol=2, widths = 1:2, 
# #              heights=unit(c(1,10), c("in", "mm")))
# grid.arrange(grobs=c(p, q2), ncol=2, widths = 2:1)
# 
# 
# # combine both plots (last should really be "pmax", it's an unfortunate bug)
# g2 <- gtable:::rbind_gtable(p, q2, "last")
# 
# # locate the panels in the gtable layout
# panels <- g2$layout$t[grepl("panel", g2$layout$name)]
# # assign new (relative) heights to the panels, based on the number of breaks
# n1 <- 2; n2 <- 2
# g2$widths[panels] <- list(unit(n1*2,"null"), unit(n2, "null")) 
# # notice the *2 here to get different heights 
# grid.newpage()
# grid.draw(g2)
# 
# don <- ggarrange(p, g, ncol = 2, nrow = 1, widths=c(2, 1), heights=c(1))
# don
# ggsave(here::here("results", "scatter6.png"), don, width=6, height=4)
# 
# layout <- rbind(c(1, 1, 1, 1, NA, 2),
#                 c(1, 1, 1, 1, NA, 2))
# grid.arrange(p, q2, layout_matrix=layout)

```



## Worst correlation differences
```{r cor_worst}

tab <- cor.comp %>%
  arrange(desc(abs(diff))) %>%
  select(-vname1, -vname2) %>%
  rename(variable_pair=combo, PUF=puf, variable1=vdesc1, variable2=vdesc2) %>%
  # filter(abs(diff)>=.1)
  head(10) 

tab %>%
  kable(digits=3) %>%
  kable_styling(full_width = FALSE)

vw <- 25
ft <- tab %>%
  mutate(variable1=str_wrap(variable1, width=vw),
         variable2=str_wrap(variable2, width=vw)) %>%
  flextable() %>% 
  set_header_labels(variable_pair="Variable pair",
                    PUF = "PUF\ncorrelation",
                    syn = "Synthetic\ncorrelation",
                    diff="Difference",
                    variable1="Variable 1",
                    variable2="Variable 2") %>%
  theme_box() %>%
  autofit() %>% # dim
  width(j=~variable1 + variable2, width=3)
ft
dim(ft)
save_as_image(ft, path = here::here("results", "corr_worst_table.png"))
save_as_image(ft, path = here::here("results", "corr_worst_table7.png"), zoom=7)
save_as_image(ft, path = here::here("results", "corr_worst_table10.png"), zoom=10)
save_as_image(ft, path = here::here("results", "corr_worst_table.pdf"))
# browseVignettes(package = "flextable")
# myft <- flextable(
#   head(mtcars), 
#   col_keys = c("am", "carb", "gear", "mpg", "drat" ))
# myft
# myft <- theme_vanilla(myft)
# myft theme_zebra
tab %>%
  flextable() %>% 
  theme_box() %>%
  autofit()

```


# Examine weighted UNenhanced files

<!--
Comments
-->

## First, test taxcalc
```{r taxcalc_test}
# Tax-Calculator user guide:
# https://pslmodels.github.io/Tax-Calculator/uguide.html

# Here is the tc CLI usage: 
# tc INPUT TAXYEAR [--help]
# [--baseline BASELINE] [--reform REFORM] [--assump  ASSUMP]
# [--exact] [--tables] [--graphs]
# [--dump] [--dvars DVARS] [--sqldb] [--outdir OUTDIR]
# [--test] [--version]

# --dump causes all the input variables (including the ones understood by Tax-Calculator but not included in test.csv,
# which are all zero) and all the output variables calculated by Tax-Calculator to be included in the output file

# Create a dumpvars.txt text file like this:
# c00100 c62100 c09600 c05800 taxbc
# cat("c00100 c62100 c09600 c05800 taxbc", file="D:/tax_data/tc_testfiles/dumpvars.txt",sep=" ")


# first baseline, then reform
# tc "d:/tcdir/puf_vs_csv/puf.csv" 2014 --dump --outdir "d:/tcdir/puf_vs_csv"
# tc "d:/tcdir/puf_vs_csv/cps.csv" 2014 --dump --outdir "d:/tcdir/puf_vs_csv"

# tc test.csv 2020 runs tc on the file test.csv; it produces test-20-#-#-#.csv
#   first # symbol indicates we did not specify a baseline file
#   second # symbol indicates we did not specify a policy reform file
#   third # symbol indicates we did not specify an economic assumption file.

# variables included in the minimal output file include:
#   RECID (of filing unit in the input file), YEAR (specified when executing tc), WEIGHT (which is same as s006),
#   INCTAX (which is same as iitax), LSTAX (which is same as lumpsum_tax) and PAYTAX (which is same as payroll_tax)

# Tax-Calculator extrapolation: ----
# Per the user guide, 
#   Tax-Calculator knows to extrapolate (or age) filing unit data in the cps.csv file to the specified tax year
# I assume this is true if the file is named puf.csv, also, but documentation does not say

# To do extrapolation manually, we will need:
#   our equivalent of the enhanced puf: puf_data/cps-matched-puf.csv (restricted so not online)
#   taxdata/puf_stage1/growfactors.csv and Tax-Calculator/taxcalc/growfactors.csv,
#     apply as in Tax-Calculator/taxcalc/records.py _extrapolate
#   our equivalent of taxdata/puf_stage2/puf_weights.csv
#   taxdata/puf_stage3/puf_ratios.csv and Tax-Calculator/taxcalc/puf_ratios.csv
#     apply as Tax-Calculator/in taxcalc/records.py _adjust
#  plus, useful to look at stage3_targets.csv but not needed


# To find directory for tc, in a command window type:
#   where tc.exe

# Build a Windows system command using the R command system2:
#   - needs to define the location of the tc.exe command
#   - to find this location, in a command window type: where tc.exe
#   - all directory and file locations must be specified fully, not relative to this project
#   - any directory or file names in the args component of system2 that have spaces must be shQuoted as shown below
#   - fine to quote them all, even if they don't have spaces, just to be safe
#   - DO NOT shQuote the cmd argument of system2

# Use system2 command: here is an example:
# cmd1 <- "C:/ProgramData/Anaconda3/Scripts/tc"
# args <- c(shQuote("D:/tcdir/synth10syn20.csv"), "2013", 
#           "--reform", "D:/Dropbox/RPrograms PC/OSPC/syndata4/tax_plans/brk4_1k_2013.json", 
#           "--dump", 
#           "--outdir", "D:/tcdir/")
# system2(cmd1, args)

# Do not include --reform and its location if this is a baseline run

test_dir <- "D:/tax_data/tc_testfiles/"
test_path <- paste0(test_dir, "test.csv")
cmd1 <- "C:/ProgramData/Anaconda3/Scripts/tc"

# very simple run, baseline
args <- c(shQuote(test_path), "2020",
          "--outdir", shQuote(test_dir))

# baseline, dump all results
args <- c(shQuote(test_path), "2020",
          "--dump",
          "--outdir", shQuote(test_dir))

# A simple run - copy the line below, from C:, exactly as is, into a command window and run:
#   C:/ProgramData/Anaconda3/Scripts/tc "D:/tax_data/tc_testfiles/test.csv" 2020 --outdir "D:/tax_data/tc_testfiles/"
# when we construct the command in R, we must shQuote the directory names, so an R variable that contains this information
# will not look quite like the commented out line above.

# create a list of variables to put in the output file, if we do not want them all
dvars <- c("c00100", "c62100", "c09600", "c05800", "taxbc")
dvars_path <- "D:/tax_data/tc_testfiles/dumpvars.txt"
cat(dvars, file=dvars_path, sep=" ") # write the dvars file

args <- c(shQuote(test_path), "2020",
          "--dump",
          "--dvars", shQuote(dvars_path),
          "--outdir", shQuote(test_dir))

cmd1; args

system2(cmd1, args)


```


## taxcalc on puf and the lowmatch file synpuf20_lowmatch_wfs.csv
```{r taxcalc_oldfiles}
# synpuf20_lowmatch
# puf2011.csv
# C:\Users\donbo\Google Drive\synpuf\syntheses\synpuf20_lowmatch.csv
# synpuf20_lowmatch_wfs.csv
# synpuf20_no_disclosures_weighted_for_taxdata.csv
# use o (for "old") prefix
# C:\Users\donbo\Dropbox\OSPC - Shared\IRS_pubuse_2011\puf2011.csv

# puf['e03260'] = 0  (values are all zero in synthetic puf)

puf_vnames <- get_puf_vnames() %>% mutate(vname=str_to_upper(vname))

# old puf
opuf <- read_csv("C:/Users/donbo/Dropbox/OSPC - Shared/IRS_pubuse_2011/puf2011.csv",
                 col_types = cols(.default= col_double()), n_max=-1)
ns(opuf)
count(opuf, MARS)
opuf %>%
  summarise_at(vars(E09600, E05800), list(~ sum(. * S006 / 100) / 1e9))

# osyn <- read_csv("C:/Users/donbo/Google Drive/synpuf/syntheses/synpuf20_lowmatch.csv", 
#                  col_types = cols(.default= col_double()), n_max=-1)
# ns(osyn)

osyn2 <- read_csv("C:/Users/donbo/Google Drive/synpuf/syntheses/synpuf20_lowmatch_wfs.csv", 
                 col_types = cols(.default= col_double()), n_max=-1)

ns(osyn2)
count(osyn2, MARS)

osyn2a <- osyn2 %>%
  rename(s006=wt.wtfs) %>%
  mutate(s006=s006 * 100) %>%
  select(-c(ftype, ends_with("p"), e00200s, e02100s, e00900s,
            m, group, mgroup, c00100, taxbc, wt, wt.syn, pufseqn, wt.puf,
            e00600_minus_e00650, e01500_minus_e01700)) %>%
  setNames(str_to_upper(names(.)))
ns(osyn2a)


syn_nd <- read_csv(syntdpath, 
                   col_types = cols(.default= col_double()), n_max=-1)
glimpse(syn_nd)

# AMTI and AMT
# e00700 Taxable refunds of state and local income taxes
# c04470 Itemized deductions after phase-out (zero for non-itemizers)
# c17000 Sch A: Medical expenses deducted (component of pre-limitation c21060 total)
# c18300 Sch A: State and local taxes plus real estate taxes deducted (component of pre-limitation c21060 total)
# c20800 Sch A: Net limited miscellaneous deductions deducted (component of pre-limitation c21060 total)
# c21040 Itemized deductions that are phased out
    # if standard == 0.0:
    #     c62100 = (c00100 - e00700 - c04470 +
    #               max(0., min(c17000, 0.025 * c00100)) +
    #               c18300 + c20800 - c21040)
    # if standard > 0.0:
    #     c62100 = c00100 - e00700



# common_names2 <- intersect(names(opuf), names(osyn2)) # we lose filer and data_source, don't think we need either
# common_names2 %>% sort

```


### taxcalc on the unenhanced puf and Max's lowmatch, with weights from scratch
```{r unenh_lowmatch}
common_names <- intersect(names(opuf), names(osyn2a)) # we lose filer and data_source, don't think we need either
common_names %>% sort

ostack <- bind_rows(opuf %>% filter(MARS!=0) %>% select(common_names) %>% mutate(ftype="puf"),
                    osyn2a %>% select(common_names) %>% mutate(ftype="syn"))

ostack_rotten <- ostack %>%
  setNames(change_case(names(.))) %>%
  do(prime_spouse_splits(.)) %>%
  arrange(ftype, RECID) %>%
  mutate(RECID_original=RECID,
         RECID=row_number())

ns(ostack_rotten)


set.seed(1234)
ostack_rotten %>%
  group_by(ftype) %>%
  sample_n(1000) %>%
  write_csv(osr_samp_path)

ostack_rotten %>%
  write_csv(osr_path)

# stackpath <- osr_samp_path
stackpath <- osr_path

cmd1 <- "C:/ProgramData/Anaconda3/Scripts/tc"
args <- c(shQuote(stackpath), "2013",
          "--dump",
          "--outdir", taxout)
cmd1
args

#.. run the command ----
a <- proc.time()
system2(cmd1, args) # CAUTION: this will overwrite any existing output file that had same input filename!
# consider system2
b <- proc.time()
b - a  # it can easily take 5-10 minutes depending on the size of the input file

year <- 2013
# (tc_osr_samp_path <- paste0(taxout, "ostack_rotten_samp-", str_sub(year, 3, 4), "-#-#-#.csv"))
(tc_osr_path <- paste0(taxout, "ostack_rotten-", str_sub(year, 3, 4), "-#-#-#.csv"))

# use either the sample or full output
# f1 <- osr_samp_path; f2 <- tc_osr_samp_path # sample output
f1 <- osr_path; f2 <- tc_osr_path # full output
df1 <- read_csv(f1)
df2 <- read_csv(f2)
ns(df1)
ns(df2)
check <- df1 %>% 
  select(ftype, RECID, RECID_original, s006) %>%
  left_join(df2 %>% select(-s006), by="RECID")
glimpse(check)
ns(check)
count(check, ftype, FLPDYR)

vars <- c("taxbc", "c09600", "c05800", "c00100", "c62100")
check %>% 
  select(ftype, FLPDYR, s006, vars) %>%
  group_by(ftype, FLPDYR) %>%
  summarise_at(vars(vars), list(~sum(s006 / 100 * .) / 1e9)) %>%
  pivot_longer(vars, names_to="vname", values_to = "value") %>%
  pivot_wider(names_from=ftype, values_from=value) %>%
  mutate(diff=syn - puf, pdiff=diff / puf * 100) %>%
  kable(format="rst", digits=1, format.args = list(big.mark=","))

```





## taxcalc on the unenhanced puf and Max's no disclosures, my new weights synpuf20_no_disclosures_weighted_for_taxdata.csv

```{r unenh_nodisc}
setdiff(names(opuf), names(syn_nd)) # a lot of puf vars not in syn
setdiff(names(syn_nd), names(opuf)) # all synvars are in puf, good
common_names <- intersect(names(opuf), names(syn_nd)) # we lose filer and data_source, don't think we need either
common_names %>% sort

ostack <- bind_rows(opuf %>% filter(MARS!=0) %>% select(common_names) %>% mutate(ftype="puf"),
                    syn_nd %>% select(common_names) %>% mutate(ftype="syn"))

ostack_rotten <- ostack %>%
  setNames(change_case(names(.))) %>%
  do(prime_spouse_splits(.)) %>%
  arrange(ftype, RECID) %>%
  mutate(RECID_original=RECID,
         RECID=row_number())
count(ostack_rotten, ftype)

ns(ostack_rotten)
ostack_rotten %>% group_by(ftype) %>% summarise(wt=sum(s006 /100))

# get weighted sums of all variables in the file
ostacklong <- ostack_rotten %>%
  select(ftype, MARS, s006, starts_with("e"), starts_with("p")) %>%
  pivot_longer(-c(ftype, MARS, s006), names_to = "vname", values_to = "value")
glimpse(ostacklong)

owtdns <- ostack_rotten %>% group_by(ftype, MARS) %>% summarise(value=sum(s006 / 100)) %>% mutate(vname="_wtdn")
ostacksum <- ostacklong %>%
  group_by(ftype, MARS, vname) %>%
  summarise_at(vars(-s006), list(~sum(. * s006/100) / 1e6)) %>%
  bind_rows(owtdns)

odiffs <- ostacksum %>%
  pivot_wider(names_from = ftype, values_from = value) %>%
  mutate(diff=syn - puf,
         pdiff=diff / puf * 100) %>%
  arrange(MARS, desc(abs(diff)))


odiffs %>%
  arrange(MARS, desc(abs(diff))) %>%
  # arrange(MARS, desc(abs(pdiff))) %>%
  group_by(MARS) %>%
  filter(row_number() <= 10) %>%
  ungroup %>%
  kable(digits=1, format.args = list(big.mark=","), format="rst")

# get total diffs
ostacksum %>%
  group_by(ftype, vname) %>%
  summarise(value=sum(value)) %>%
  pivot_wider(names_from = ftype, values_from = value) %>%
  mutate(diff=syn - puf,
         pdiff=diff / puf * 100)  %>%
  arrange(desc(abs(diff))) %>%
  # arrange(desc(abs(pdiff))) %>%
  filter(row_number() <= 20) %>%
  left_join(puf_vnames %>% mutate(vname=str_to_lower(vname)) %>% select(vname, vdesc)) %>%
  kable(digits=1, format.args = list(big.mark=","), format="rst")
  

# set.seed(1234)
# ostack_rotten %>%
#   group_by(ftype) %>%
#   sample_n(1000) %>%
#   write_csv("C:/Users/donbo/Dropbox/SLGF/taxdata_synpuf/puf_synnd_stack_rotten_samp.csv")

ostack_rotten %>%
  write_csv("C:/Users/donbo/Dropbox/SLGF/taxdata_synpuf/puf_synnd_stack_rotten.csv")

stackpath <- "C:/Users/donbo/Dropbox/SLGF/taxdata_synpuf/puf_synnd_stack_rotten.csv"

year <- 2017

cmd1 <- "C:/ProgramData/Anaconda3/Scripts/tc"
args <- c(shQuote(stackpath), year,
          "--dump",
          "--outdir", taxout)
cmd1
args

#.. run the command ----
a <- proc.time()
system2(cmd1, args) # CAUTION: this will overwrite any existing output file that had same input filename!
# consider system2
b <- proc.time()
b - a  # it can easily take 5-10 minutes depending on the size of the input file

(tc_path <- paste0(taxout, "puf_synnd_stack_rotten-", str_sub(year, 3, 4), "-#-#-#.csv"))

# use either the sample or full output
# f1 <- osr_samp_path; f2 <- tc_osr_samp_path # sample output
f1 <- stackpath; f2 <- tc_path # full output
df1 <- read_csv(f1)
df2 <- read_csv(f2)
ns(df1)
ns(df2)
check <- df1 %>% 
  select(ftype, RECID, RECID_original, s006) %>%
  left_join(df2 %>% select(-s006), by="RECID")
glimpse(check)
ns(check)
count(check, ftype, FLPDYR)

check %>% group_by(ftype) %>% summarise(wt=sum(s006 /100))

vars <- c("taxbc", "c09600", "c05800", "c00100", "c62100")
check %>% 
  select(ftype, FLPDYR, s006, vars) %>%
  group_by(ftype, FLPDYR) %>%
  summarise_at(vars(vars), list(~sum(s006 / 100 * .) / 1e9)) %>%
  pivot_longer(vars, names_to="vname", values_to = "value") %>%
  pivot_wider(names_from=ftype, values_from=value) %>%
  mutate(diff=syn - puf, pdiff=diff / puf * 100) %>%
  kable(format="rst", digits=1, format.args = list(big.mark=","))

```


# Tax calc on the ENHANCED files from Yimeng: puf_rottenpuf.csv puf_synpuf.csv
## Get the data files
```{r}
puf_vnames <- get_puf_vnames() %>% mutate(vname=str_to_upper(vname))

epuf <- read_csv(epuf_path, col_types = cols(.default= col_double()), n_max=-1)
esyn <- read_csv(esyn_path, col_types = cols(.default= col_double()), n_max=-1)

nrow(epuf); nrow(esyn) # 248591 762749
ncol(epuf); ncol(esyn) # 87 each
setdiff(names(epuf), names(esyn)) # epuf has filer, not in esyn
setdiff(names(esyn), names(epuf)) # esyn has data_source, not in epuf
count(epuf, filer)
#   filer      n
#   <dbl>  <int>
# 1     0   7346
# 2     1 241245
count(esyn, data_source) # interesting...
#   data_source      n
#         <dbl>  <int>
# 1           0   7346
# 2           1 755403
# I don't think we need either of the variables

```


## check a few variables
```{r}
ns(epuf)
ns(esyn)

tmp <- bind_rows(epuf %>% mutate(ftype="puf"),
                 esyn %>% mutate(ftype="syn"))
tmp %>%
  group_by(ftype) %>%
  summarise(n=n(), wtdn=sum(s006 / 100), wages=sum(e00200 * s006 / 100) / 1e6, minFLPDYR=min(FLPDYR), maxFLPDYR=max(FLPDYR))



```


## Stack and make the epuf and esyn files similiarly rotten
```{r}
# what do the split variables look like

common_names <- intersect(names(epuf), names(esyn)) # we lose filer and data_source, don't think we need either
estack <- bind_rows(epuf %>% select(common_names) %>% mutate(ftype="puf"),
                    esyn %>% select(common_names) %>% mutate(ftype="syn")) %>%
  select(-FLPDYR) %>%
  arrange(ftype, RECID) %>%
  mutate(RECID_original=RECID,
         RECID=row_number())

split_check <- estack %>%
  filter(ftype=="puf", e00900!=0) %>%
  group_by(MARS) %>%
  sample_n(100) %>%
  select(RECID, MARS, e00900, e00900p, e00900s)
# for non-married records, the entire value of e00900 is in e00900p

bad <- estack %>% 
  filter(e00200 != (e00200p + e00200s)) %>% 
  select(ftype, RECID, starts_with("e00200")) %>%
  mutate(ppct=e00200p / e00200 * 100,
         spct=e00200s / e00200 * 100,
         pctsum=ppct + spct)
count(bad, ftype) # all are syn records

# rotten enhanced file, same rules for both
estack_rotten <- estack %>%
  # first fix the e00200 variables because 541 of these records on the synfile do not sum properly
  # within Tax-Calculator tolerances although most are extremely close so choose a simple fix
  mutate(e00200s=e00200 - e00200p) %>%
  # now fix the truly rotten variables
  mutate(e00900p=case_when(MARS==2 & e00900!=0 & e00200!=0 ~ e00200p / e00200 * e00900,
                           MARS==2 & e00900!=0 & e00200==0 ~ e00900 * .5,
                           MARS!=2 ~ e00900,
                           TRUE ~ e00900p),
         e00900s=e00900 - e00900p,
         e02100p=case_when(MARS==2 & e02100!=0 & e00200!=0 ~ e00200p / e00200 * e02100,
                           MARS==2 & e02100!=0 & e00200==0 ~ e02100 * .5,
                           MARS!=2 ~ e02100,
                           TRUE ~ e02100p),
         e02100s=e02100 - e02100p)

esr_samp_path <- paste0(slgfdir, "estack_rotten_samp.csv")
esr_path <- paste0(slgfdir, "estack_rotten.csv")

set.seed(1234)
estack_rotten %>%
  group_by(ftype) %>%
  sample_n(1000) %>%
  write_csv(esr_samp_path)

estack_rotten %>%
  group_by(ftype) %>%
  write_csv(esr_path)

# details below
set.seed(1234)
estack_rotten %>%
  filter(e00900p!=0) %>%
  group_by(ftype) %>%
  sample_n(30) %>%
  select(ftype, MARS, RECID, starts_with("e00900"), starts_with("e00200")) %>%
  arrange(MARS, ftype, RECID)

set.seed(1234)
estack_rotten %>%
  filter(e02100p!=0) %>%
  group_by(ftype) %>%
  sample_n(30) %>%
  select(ftype, MARS, RECID, starts_with("e02100"), starts_with("e00200")) %>%
  arrange(MARS, ftype, RECID)


# various checks below to help me figure out how to make the files similarly rotten
common_names %>% sort # the only X variable is XTOT

# record weight on both files appears to need to be divided by 100, but we'll bring in weights later so don't worry now
quantile(epuf$s006)
quantile(esyn$s006)

# e00900 syn file has 0 for all
estack %>%
  filter(MARS==2, e00900!=0) %>%
  group_by(ftype) %>%
  mutate(n=n()) %>%
  summarise_at(vars(n, e00900, e00900p, e00900s), list(mdn=~median(.), max=~max(.)))

# e02100 syn file problems are less clear
estack %>%
  filter(MARS==1, e02100!=0) %>%
  group_by(ftype) %>%
  mutate(n=n()) %>%
  summarise_at(vars(n, e02100, e02100p, e02100s), list(mdn=~median(.), max=~max(.)))



count(estack, ftype, XTOT) # looks plausible

```


### get weighted sums of all variables in the estack_rotten file
```{r}
# estack_rotten
glimpse(estack_rotten)
estacklong <- estack_rotten %>%
  select(ftype, MARS, s006, starts_with("e"), starts_with("p")) %>%
  pivot_longer(-c(ftype, MARS, s006), names_to = "vname", values_to = "value")
glimpse(estacklong)

wtdns <- estack_rotten %>% group_by(ftype, MARS) %>% summarise(value=sum(s006 / 100)) %>% mutate(vname="_wtdn")

estacksum <- estacklong %>%
  group_by(ftype, MARS, vname) %>%
  summarise_at(vars(-s006), list(~sum(. * s006/100) / 1e6)) %>%
  bind_rows(wtdns)
glimpse(estacksum)

ediffs <- estacksum %>%
  pivot_wider(names_from = ftype, values_from = value) %>%
  mutate(diff=syn - puf,
         pdiff=diff / puf * 100) %>%
  arrange(MARS, desc(abs(diff)))


ediffs %>%
  arrange(MARS, desc(abs(diff))) %>%
  # arrange(MARS, desc(abs(pdiff))) %>%
  left_join(puf_vnames %>% mutate(vname=str_to_lower(vname)) %>% select(vname, vdesc)) %>%
  group_by(MARS) %>%
  filter(row_number() <= 10) %>%
  ungroup %>%
  kable(digits=1, format.args = list(big.mark=","), format="rst")

# get file sums not by MARS
tmp <- estacklong %>%
  select(-MARS) %>%
  group_by(ftype, vname) %>%
  summarise_at(vars(-s006), list(~sum(. * s006/100) / 1e6)) %>%
  pivot_wider(names_from = ftype, values_from = value) %>%
  mutate(diff=syn - puf,
         pdiff=diff / puf * 100) %>%
  arrange(desc(abs(diff)))
tmp %>% 
  filter(row_number() <= 10) %>%
  left_join(puf_vnames %>% mutate(vname=str_to_lower(vname)) %>% select(vname, vdesc)) %>%
  kable(digits=1, format.args = list(big.mark=","), format="rst")


```



### Run tax calculator
```{r taxcalc_enhanced}
esr_samp_path <- paste0(slgfdir, "estack_rotten_samp.csv")
esr_path <- paste0(slgfdir, "estack_rotten.csv")
esr_samp_path
esr_path

year <- 2017
stackpath <- esr_path
cmd1 <- "C:/ProgramData/Anaconda3/Scripts/tc"
args <- c(shQuote(stackpath), year,
          "--dump",
          "--outdir", taxout)
cmd1
args

#.. run the command ----
a <- proc.time()
system2(cmd1, args) # CAUTION: this will overwrite any existing output file that had same input filename!
# consider system2
b <- proc.time()
b - a  # it can easily take 5-10 minutes depending on the size of the input file

esr_path
(tc_path <- paste0(taxout, "estack_rotten-", str_sub(year, 3, 4), "-#-#-#.csv"))

f1 <- stackpath; f2 <- tc_path # full output
df1 <- read_csv(f1)
df2 <- read_csv(f2)
ns(df1)
ns(df2)
check <- df1 %>% 
  select(ftype, RECID, RECID_original, s006) %>%
  left_join(df2 %>% select(-s006), by="RECID")
glimpse(check)
ns(check)
count(check, ftype, FLPDYR)

check %>% group_by(ftype) %>% summarise(wt=sum(s006 /100))

vars <- c("taxbc", "c09600", "c05800", "c00100", "c62100")
check %>% 
  select(ftype, FLPDYR, s006, vars) %>%
  group_by(ftype, FLPDYR) %>%
  summarise_at(vars(vars), list(~sum(s006 / 100 * .) / 1e9)) %>%
  pivot_longer(vars, names_to="vname", values_to = "value") %>%
  pivot_wider(names_from=ftype, values_from=value) %>%
  mutate(diff=syn - puf, pdiff=diff / puf * 100) %>%
  kable(format="rst", digits=1, format.args = list(big.mark=","))


## STUFF BELOW IS NOT UPDTATED 2019-11-14 ----
# now bring in weights -- SOON - VERIFY that they are in same order as original files ----
library("R.utils")

# the PUF weights (rotten)
epz <- gzfile(paste0(slgfdir, ep_weights_gz), open="rb")  
epw <- read_csv(file=epz, col_types = cols(.default= col_double()))
glimpse(epw) # 248,591 - good

# the synthetic file weights (rotten)
esz <- gzfile(paste0(slgfdir, es_weights_gz), open="rb")  
esw <- read_csv(file=esz, col_types = cols(.default= col_double()))
glimpse(esw) # 762,749 good

glimpse(check)
check2 <- check %>%
  arrange(ftype, RECID_original) %>%
  mutate(wt=c(epw$WT2018, esw$WT2018))
ht(check2)

check2 %>% 
  group_by(ftype) %>%
  summarise(s006=sum(s006), wt=sum(wt)) %>%
  pivot_longer(c(s006, wt), names_to="vname", values_to = "value") %>%
  pivot_wider(names_from=ftype, values_from=value) %>%
  mutate(diff=syn - puf, pdiff=diff / puf * 100) %>%
  kable(format="rst", digits=1, format.args = list(big.mark=",")) 

check2 %>% 
  group_by(ftype) %>%
  summarise_at(vars(taxbc, c09600, c05800), list(~sum(wt / 100 * .) / 1e9)) %>%
  pivot_longer(c(taxbc, c09600, c05800), names_to="vname", values_to = "value") %>%
  pivot_wider(names_from=ftype, values_from=value) %>%
  mutate(diff=syn - puf, pdiff=diff / puf * 100) %>%
  kable(format="rst", digits=1, format.args = list(big.mark=","))

check2 %>% 
  group_by(ftype) %>%
  # this shows that the problem is with synthesis, not weighting - possibly in the values that go into c09600
  # mutate(s006=1000) %>% 
  summarise_at(vars(taxbc, c09600, c05800), list(~sum(s006 / 100 * .) / 1e9)) %>%
  pivot_longer(c(taxbc, c09600, c05800), names_to="vname", values_to = "value") %>%
  pivot_wider(names_from=ftype, values_from=value) %>%
  mutate(diff=syn - puf, pdiff=diff / puf * 100) %>%
  kable(format="rst", digits=1, format.args = list(big.mark=","))

```


## Temporary work with puf and syn
```{r}
glimpse(stack)
ns(stack)
count(stack, ftype, XOCAH)

stack2 <- stack %>%
  setNames(change_case(names(.))) %>% # Tax-Calculator expects mostly lower-case names
  do(impose_variable_rules(.)) %>% # not needed for synpuf5 and later
  do(prime_spouse_splits(.)) %>%
  arrange(ftype) %>% # DJB NEW!
  mutate(RECID_original=RECID,
         RECID=row_number()) # WILL OVERWRITE OLD RECID!!!!

glimpse(stack2)

set.seed(1234)
slim <- 
  stack2 %>%
  group_by(ftype) %>%
  sample_n(10e3)

stack2 %>% write_csv(paste0(taxout, "tcbase.csv"))
slim %>% write_csv(paste0(taxout, "tcbase_slim.csv"))

# use one of these
stackpath <- "D:/tax_data/tcbase.csv"
stackpath <- "D:/tax_data/tcbase_slim.csv"

cmd1 <- "C:/ProgramData/Anaconda3/Scripts/tc"
args <- c(shQuote(stackpath), "2018",
          "--dump",
          "--outdir", taxout)
cmd1
args

#.. run the command ----
a <- proc.time()
system2(cmd1, args) # CAUTION: this will overwrite any existing output file that had same input filename!
# consider system2
b <- proc.time()
b - a  # it can easily take 5-10 minutes depending on the size of the input file

```


```{r stub_parallel}
# ONETIME for each new combination of files making up stack ----
# IF NEEDED take the already prepared stack file and write it to a directory that all parallel processes can read ----
write_csv(stack, paste0(globals$tc.dir, source.fn))
saveRDS(stack, paste0(globals$tc.dir, str_remove(source.fn, ".csv"), "_stack.rds"))
# run the baseline tax on the stacked file
run_taxplan_getresults(source.fn) # write output to csv and rds files
# END ONETIME ----

#.. run it in parallel ----
library("doParallel")
cl <- makeCluster(6)
registerDoParallel(cl)

packages <- c("magrittr", "tidyverse", "dplyr")
# CAUTION:  DO NOT PASS large items as function arguments
# instead export them
xport <- c("globals", "source.fn", "run_taxplan_getresults", "tc.wincmd") 
popts <- list(.packages=packages, .export=xport)
popts

# now run in parallel
a <- proc.time()
warn <- options()$warn
options(warn=-1)
l_ply(reform, .progress="text", .parallel=FALSE, .paropts=popts, .fun=runtc, source.fn=source.fn)
options(warn=warn)
b <- proc.time()
b - a

stopCluster(cl)

# conditional execution in a pipeline - for parallel or not
# https://community.rstudio.com/t/conditional-pipelines/6076/2
# mtcars %>% 
#   {if (FALSE) filter(., hp == 245) else .} %>% 
#   {if (FALSE) select(., cyl) else .} %>%
#   head(10)

# choose one of the following
ftype_opt <- "syn"
# ftype_opt <- "syn_nd"
parallel <- TRUE
# parallel <- FALSE

# djb ----
a <- proc.time()
if(parallel){
  # set the latest versions of functions, etc. up for the run
  cluster_copy(cluster, c("getwts_ipopt", "ipopt_reweight",
                          "define_jac_g_structure_dense", "eval_f_xtop", "eval_grad_f_xtop", "eval_g_dense",
                          "eval_jac_g_dense", "eval_h_xtop",
                          "calc_constraints",
                          "bounds"))
  cluster_library(cluster, c("dplyr", "tidyr", "ipoptr", "readr"))
}
opt_pre <- ccoef %>%
  filter(ftype==ftype_opt) %>%
  # filter(ugroup %in% 6) %>%
  group_by(ugroup) %>%
  {if (parallel) partition(., cluster) else .}

opt <- opt_pre %>%
  do(getwts_ipopt(., all_constraint_vals, bounds, scale=TRUE)) %>%
  {if (parallel) collect(.) else .} %>%
  ungroup
b <- proc.time()
b - a # seconds
(b - a) / 60 # minutes

# https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/skewed-distribution/
# https://www.frontiersin.org/articles/10.3389/fpsyg.2019.01089/full  overlap
# https://www.jclinepi.com/article/S0895-4356(10)00257-X/pdf
# 


```



## Create comparable data
```{r notes, eval=FALSE}
# Nov 7 2019 email from Yimeng:

# I have put the files you requested in the shared dropbox folder SLGF\taxdata_synpuf.
# (I am not authorized to write in the folder you shared the synthetic puf files)
# There are six files
# 
# (1). cps-matched-puf.csv: the original puf file without any modifications. 
# (2). puf_weights_CVXOPT_puf.csv.gz, weights solved with all constraints, using (1) as input. 
# 
# (3) cps-matched-puf_synpuf.csv, synthetic puf, with all 'XO***' variables set to value 1. 
# (4) puf_weights_CVXOPT_synpuf.csv.gz: weights solved with the constraint "dependent_exempt_num " removed, using (3) as input
# 
# (5) cps-matched-puf_rottenpuf.csv: the original puf file "rotten" the same way as the synthetic puf. Following modifications are made of puf2011.csv before going through the matching process:
#     puf['xocah']  = 1
#     puf['xocawh'] = 1
#     puf['xoodep'] = 1
#     puf['xopar']  = 1
#     puf['e03260'] = 0  (values are all zero in synthetic puf)
# (6) puf_weights_CVXOPT_rottenpuf.csv.gz: weights solved with the constraint "dependent_exempt_num " removed, using (6) as input
# 
# Weights for all years are included. The flies for weights are compressed (.gz) and you need to unzip them before using. (Can be unzipped by 7-zip, the gunzip function in R.utils package can also do it)

# DJB note 11/9/2019: the description above is not quite accurate. The files that Yimeng calls "original" are the enhanced versions
# after having been run through TaxData. This is what we want.

```


```{r name_defs, include=FALSE}


ep_rotten <- "cps-matched-puf_rottenpuf.csv" # the true PUF as enhanced, with "rotten" features added
ep_weights_gz <- "puf_weights_CVXOPT_rottenpuf.csv.gz" # weights for above
ep_weights <- "puf_weights_CVXOPT_rottenpuf.csv" # weights for above
#              puf_weights_CVXOPT_rottenpuf.csv.gz

# es_rotten <- "cps-matched-puf_synpuf.csv"  # the synpuf rotten as is (with X) values set to 1
es_rotten <- "puf_synpuf.csv"
es_weights_gz <- "puf_weights_CVXOPT_synpuf.csv.gz" # weights for above
es_weights <- "puf_weights_CVXOPT_synpuf.csv" # weights for above


eptruefn <- "puf_true_input_for_tc.csv" # the true input for tc -- in the taxout directory

```


```{r get_enhanced, include=FALSE}
eptrue <- read_csv(paste0(taxout, eptruefn))
# ,                         col_types = cols(.default= col_double()),                         n_max=-1)
glimpse(eptrue)

# get TaxData-enhanced data files
epuf <- read_csv(paste0(slgfdir, ep_rotten),
                         col_types = cols(.default= col_double()),
                         n_max=-1)

# puf_synpuf.csv

ns(epuf)

nrow(eptrue); nrow(epuf)
ncol(eptrue); ncol(epuf)
ns(eptrue)


esyn <- read_csv(paste0(slgfdir, es_rotten),
                         col_types = cols(.default= col_double()),
                         n_max=-1)
ns(esyn)
nrow(esyn); ncol(esyn)

# check names
setdiff(names(epuf), names(esyn)) # "f8867"  "f8949"  "e59720" "e11601" "e11602" "e11603" "e25550"
setdiff(names(esyn), names(epuf)) # none
puf_not_syn <- c("f8867", "f8949", "e59720", "e11601", "e11602", "e11603", "e25550")
rotten_vars <- c("e03260", "xocah", "xocawh", "xoodep", "xopar")

puf_vnames %>%
  filter(vname %in% str_to_upper(puf_not_syn)) %>%
  select(-vnum)

puf_vnames %>%
  filter(vname %in% str_to_upper(rotten_vars)) %>%
  select(-vnum)

# variables that are in the puf but not in our data -- plus e32670
# vname vdesc category
# E11601	Total Refundable Credits Used to Offset Income Tax Before Credits	payments		
# E11602	Total Refundable Credits Used to Offset All Other Taxes	payments		
# E11603	Total Refundable Credits Refundable Parts	payments		
# E25550	Total Depreciation and Depletion of all Property	schedE		
# E59720	EIC refundable portion	payments	
# F8867	Form 8867, Paid Preparer’s Earned Income Credit Checklist	manual		
# F8949	Form 8949, Sales and Other Dispositions of Capital Assets	manual	

# "Rotten" variables -- not synthesized
# vname vdesc category
# E03260	Deduction for self-employment tax	stat_adjust		
# XOCAH	Exemptions for Children Living at Home	manual		
# XOCAWH	Exemptions for Children Living Away from Home	manual		
# XOODEP	Exemptions for Other Dependents	manual		
# XOPAR	Exemptions for Parents Living at Home or Away from Home	manual	

```

```{r get_weights, include=FALSE}
library("R.utils")

# the PUF weights (rotten)
epz <- gzfile(paste0(slgfdir, ep_weights_gz), open="rb")  
epw <- read_csv(file=epz, col_types = cols(.default= col_double()))
glimpse(epw)

# the synthetic file weights (rotten)
esz <- gzfile(paste0(slgfdir, es_weights_gz), open="rb")  
esw <- read_csv(file=esz, col_types = cols(.default= col_double()))
glimpse(esw)

```


```{r test_tc, include=FALSE}
# Tax-Calculator user guide:
# https://pslmodels.github.io/Tax-Calculator/uguide.html

# Use system2 command: here is an example:
# cmd1 <- "C:/Users/donbo/Anaconda3/Scripts/tc"
# args <- c("D:/tcdir/synth10syn20.csv", "2013", 
#           "--reform", "D:/Dropbox/RPrograms PC/OSPC/syndata4/tax_plans/brk4_1k_2013.json", 
#           "--dump", 
#           "--outdir", "D:/tcdir/")
# system2(cmd1, args)

# Do not include --reform and its location if this is a baseline run

# paste0(slgfdir, ep_rotten)
# "C:/Users/donbo/Dropbox/SLGF/taxdata_synpuf/cps-matched-puf_rottenpuf.csv"
eprpath <- "C:/Users/donbo/Dropbox/SLGF/taxdata_synpuf/cps-matched-puf_rottenpuf.csv" # rotten input

taxout <- "D:/tax_data/"
eptpath <- "D:/tax_data/puf_true_input_for_tc.csv" # the true puf input

# try to make it work with es_rotten
slgfdir <- "C:/Users/donbo/Dropbox/SLGF/taxdata_synpuf/"
(espath <- paste0(slgfdir, es_rotten))

# inspect the file enhanced synthetic rotten file
esr <- read_csv(espath,
                col_types = cols(.default= col_double()),
                n_max=-1)
glimpse(esr)
# check splits
#   e00200 = e00200p + e00200s  # wages
#   e00900 = e00900p + e00900s  # business income or loss
#   e02100 = e02100p + e02100s  # Schedule F net profit/loss
check <- esr %>%
  select(RECID, MARS, starts_with("e00200"), starts_with("e00900"), starts_with("e02100")) %>%
  mutate(chk200=e00200 - e00200p - e00200s,
         chk900=e00900 - e00900p - e00900s,
         chk2100=e02100 - e02100p - e02100s)

check %>%
  select(RECID, MARS, starts_with("chk")) %>%
  gather(vname, value, -RECID, -MARS) %>%
  group_by(MARS, vname) %>%
  do(qtiledf(.$value))

check2 <- check %>%
  filter(MARS==2)


cmd1 <- "C:/ProgramData/Anaconda3/Scripts/tc"
args <- c(shQuote(espath), "2018",
          "--dump",
          "--outdir", taxout)
cmd1
args

# test <- read_csv("C:/Windows/System32/test.csv")
# names(test)
# anyDuplicated(esr$RECID)
# ns(esr)

testfn <- paste0(taxout, "syntest.csv")
esr %>%
  # sample_n(1000) %>%
  do(prime_spouse_splits(.)) %>%
  # select(names(test)) %>%
  write_csv(testfn)
args <- c(shQuote(testfn), "2018",
          "--dump",
          "--outdir", taxout)

#.. run the command ----
a <- proc.time()
system2(cmd1, args) # CAUTION: this will overwrite any existing output file that had same input filename!
# consider system2
b <- proc.time()
b - a  # it can easily take 5-10 minutes depending on the size of the input file


# stack2 <- stack %>%
#   setNames(change_case(names(.))) %>% # Tax-Calculator expects mostly lower-case names
#   do(impose_variable_rules(.)) %>% # not needed for synpuf5 and later
#   do(prime_spouse_splits(.)) %>%
#   arrange(ftype) %>% # DJB NEW!
#   mutate(RECID_original=RECID,
#          RECID=row_number()) # WILL OVERWRITE OLD RECID!!!!


```



# compare many files selected variables
```{r}
# convert all variables to lower case
# vars <- c("taxbc", "c09600", "c05800", "c00100", "c62100")
vars <- c("XTOT", "e00100", "e00400", "e04600", "e09600", "c00100", "e00400", "e18400", "e18500", "c62100", "c09600", "taxbc")

#.. file1, the original lowwatch files, unenhanced ----
ostack_rotten
glimpse(ostack_rotten)
change_case

common_names1 <- intersect(names(opuf), names(osyn2a))
common_names1 %>% sort
file1 <- bind_rows(opuf %>% filter(MARS!=0) %>% select(common_names1) %>% mutate(ftype="puf"),
                   osyn2a %>% select(common_names1) %>% mutate(ftype="syn")) %>%
  setNames(change_case(names(.))) %>%
  do(prime_spouse_splits(.)) %>%
  arrange(ftype, RECID) %>%
  mutate(RECID_original=RECID,
         RECID=row_number()) %>%
  select(ftype, weight=s006, one_of(vars)) %>%
  mutate(grp="lowmatch",
         etype="unenhanced",
         fname=ifelse(ftype=="syn", "synpuf20_lowmatch_wfs.csv", "puf2011.csv"))
glimpse(file1)

sums1 <- file1 %>%
  group_by(grp, etype, ftype, fname) %>%
  summarise_at(vars(-c(weight)), list(~sum(. * weight / 100) / 1e6)) %>%
  pivot_longer(-c(grp, etype, ftype, fname))
sums1

#.. file2, the original no disclosure files, unenhanced ----
# synpuf20_no_disclosures_weighted_for_taxdata.csv
common_names2 <- intersect(names(opuf), names(syn_nd))
common_names2 %>% sort
file2 <- bind_rows(opuf %>% filter(MARS!=0) %>% select(common_names2) %>% mutate(ftype="puf"),
                   syn_nd %>% select(common_names2) %>% mutate(ftype="syn")) %>%
  setNames(change_case(names(.))) %>%
  do(prime_spouse_splits(.)) %>%
  arrange(ftype, RECID) %>%
  mutate(RECID_original=RECID,
         RECID=row_number()) %>%
  select(ftype, weight=s006, one_of(vars)) %>%
  mutate(grp="nodisclosures",
         etype="unenhanced",
         fname=ifelse(ftype=="syn", "synpuf20_no_disclosures_weighted_for_taxdata.csv", "puf2011.csv"))
glimpse(file2)

sums2 <- file2 %>%
  group_by(grp, etype, ftype, fname) %>%
  summarise_at(vars(-c(weight)), list(~sum(. * weight / 100) / 1e6)) %>%
  pivot_longer(-c(grp, etype, ftype, fname))
sums2

#.. file3, the no disclosure files, enhanced ----
epuf_path
esyn_path
common_names3 <- intersect(names(epuf), names(esyn))
common_names3 %>% sort
file3 <- bind_rows(epuf %>% select(common_names3) %>% mutate(ftype="puf"),
                   esyn %>% select(common_names3) %>% mutate(ftype="syn")) %>%
  select(-FLPDYR) %>%
  arrange(ftype, RECID) %>%
  mutate(RECID_original=RECID,
         RECID=row_number()) %>%
  # first fix the e00200 variables because 541 of these records on the synfile do not sum properly
  # within Tax-Calculator tolerances although most are extremely close so choose a simple fix
  mutate(e00200s=e00200 - e00200p) %>%
  # now fix the truly rotten variables
  mutate(e00900p=case_when(MARS==2 & e00900!=0 & e00200!=0 ~ e00200p / e00200 * e00900,
                           MARS==2 & e00900!=0 & e00200==0 ~ e00900 * .5,
                           MARS!=2 ~ e00900,
                           TRUE ~ e00900p),
         e00900s=e00900 - e00900p,
         e02100p=case_when(MARS==2 & e02100!=0 & e00200!=0 ~ e00200p / e00200 * e02100,
                           MARS==2 & e02100!=0 & e00200==0 ~ e02100 * .5,
                           MARS!=2 ~ e02100,
                           TRUE ~ e02100p),
         e02100s=e02100 - e02100p) %>%
  select(ftype, weight=s006, one_of(vars)) %>%
  mutate(grp="nodisclosures",
         etype="enhanced",
         fname=ifelse(ftype=="syn", "puf_synpuf.csv", "puf_rottenpuf.csv"))
glimpse(file3)

sums3 <- file3 %>%
  group_by(grp, etype, ftype, fname) %>%
  summarise_at(vars(-c(weight)), list(~sum(. * weight / 100) / 1e6)) %>%
  pivot_longer(-c(grp, etype, ftype, fname))
sums3


#.. file4, the no disclosure files, enhanced, through Tax-Calculator, 2013 law ----
esr_path
(tc_path <- paste0(taxout, "estack_rotten-", str_sub(year, 3, 4), "-#-#-#.csv"))
df1 <- read_csv(esr_path)
df2 <- read_csv(tc_path)

file4 <- df1 %>% 
  select(ftype, RECID, RECID_original, s006) %>%
  left_join(df2 %>% select(-s006), by="RECID") %>%
  select(ftype, weight=s006, one_of(vars)) %>%
  mutate(grp="nodisclosures",
         etype="enhanced_tax2013_wtsfile",
         fname=ifelse(ftype=="syn", "puf_synpuf.csv", "puf_rottenpuf.csv"))
glimpse(file4)

sums4 <- file4 %>%
  group_by(grp, etype, ftype, fname) %>%
  summarise_at(vars(-c(weight)), list(~sum(. * weight / 100) / 1e6)) %>%
  pivot_longer(-c(grp, etype, ftype, fname))
sums4

#.. combine and examine ----
grpall <- bind_rows(sums1, sums2, sums3, sums4) %>% rename(vname=name) %>% ungroup
grpall %>% filter(vname=="XTOT")

vlab_sort <- c("agi", "XTOT", "e00400", "e18400", "e18500", "AMTI", "AMT")
etype_sort <- c("unenhanced", "enhanced", "enhanced_tax2013_wtsfile")
grp_comp <- grpall %>%
  select(-fname) %>%
  pivot_wider(names_from="ftype") %>%
  mutate(diff=puf - syn,
         pdiff=diff / puf * 100) %>%
  left_join(puf_vnames %>% mutate(vname=change_case(vname)) %>% select(vname, vdesc)) %>%
  mutate(vlab=case_when(vname %in% c("c00100", "e00100") ~ "agi",
                        vname=="c09600" ~ "AMT",
                        vname=="c62100" ~ "AMTI",
                        TRUE ~ vname)) %>%
  mutate(vlab=factor(vlab, levels=vlab_sort),
         etype=factor(etype, levels=etype_sort)) %>%
  arrange(vlab, grp, etype) %>%
  select(vlab, grp, etype, puf, syn, diff, pdiff, vdesc, vname)

grp_comp %>%
  kable(digits=2, format.args = list(big.mark=","), format="rst")


```


# What happened to e00400 between the lowmatch and nodisclosures files, unenhanced? with weighting added $16b!!
```{r}
# how do the unweighted files compare??
# osyn2a vs syn_nd
glimpse(osyn2a)
glimpse(syn_nd)
setdiff(names(osyn2a), names(syn_nd))

stack <- osyn2a %>% mutate(ftype="lowmatch") %>%
  bind_rows(syn_nd %>% mutate(ftype="nodisclosures")) %>%
  mutate(income=E00200 + E00300 + E00600 + E01700) %>%
  select(RECID, income, E00200, E00400, S006, MARS, ftype)
glimpse(stack)

stack %>%
  group_by(ftype) %>%
  summarise(E00200wtd=sum(E00200 * S006) / 100 / 1e6,
            E00400wtd=sum(E00400 * S006) / 100 / 1e6,
            E00200=sum(E00200),
            E00400=sum(E00400)) %>%
  pivot_longer(-ftype) %>%
  pivot_wider(names_from = ftype) %>%
  mutate(diff=nodisclosures - lowmatch,
         pdiff=diff / lowmatch * 100)

# break it down by income range
ycuts <- c(-Inf, 0, 50e3, 100e3, 200e3, 500e3, 1e6, Inf)
stack %>%
  mutate(ygroup=cut(income, ycuts)) %>%
  group_by(ftype, ygroup) %>%
  summarise(wtdn=sum(S006 / 100),
            E00200wtd=sum(E00200 * S006) / 100 / 1e6,
            E00400wtd=sum(E00400 * S006) / 100 / 1e6) %>%
  pivot_longer(-c(ftype, ygroup)) %>%
  pivot_wider(names_from = ftype) %>%
  mutate(diff=nodisclosures - lowmatch,
         pdiff=diff / lowmatch * 100) %>%
  select(name, ygroup, everything()) %>%
  arrange(name, ygroup)



```

